# CloudTech Study

## 21.

- 3 つのカテゴリーそれぞれに対してモデルレジストリコレクションを作成します。既存のモデルグループをコレクションに移動します。
  - モデルレジストリコレクションを使用すると、既存のモデルグループをそのまま維持しながら、カテゴリーごとに整理できます。
  - このアプローチではモデルアーティファクトの整合性が保たれ、既存のグループ分けにも影響を与えません。
  - また、検索性が大幅に向上し、多くのモデルを効率的に管理できます。
- 3 つのカテゴリーそれぞれに対してカスタムタグを作成
  - カスタムタグは簡単にモデルを分類する手段を提供しますが、タグ付けの管理は分散しがちで、一貫性を保つのが難しくなります。
- カテゴリーごとにモデルグループを作成
  - モデルグループの再編成は、既存のグループ構造を変更する可能性があり、モデルアーティファクトの合性に影響を与えるリスクがあります。
- SageMaker ML Lineage Tracking を使用して、どのモデルグループにモデルを含めるべきかを自動的に識別し、タグ付けします

  - ML Lineage Tracking はモデルのプロセスや依存関係を追跡するためのものであり、モデルの分類や検索性の向上には直接的に役立ちません。

- モデルレジストリ（SageMaker Model Registry）
  - 役割：トレーニング済みの ML モデルを管理・バージョン管理する機能
  - なぜ使う？：ML エンジニアがチームのモデルを整理し、どのバージョンをデプロイすべきかを把握しやすくするため
  - ポイント：
    - SageMaker Model Registry に登録されたモデルは「モデルグループ」に分類される
    - 例：「画像認識用モデル」「NLP モデル」などのグループを作成可能
- モデルレジストリコレクション（Model Registry Collection）
  - 役割：複数のモデルグループをさらに大きなカテゴリとしてまとめる
  - なぜ使う？：数百ものモデルを検索しやすくするため
- 現在の状態（整理されていない）
  - 📂 SageMaker Model Registry
    - └ 📁 モデルグループ A
    - └ 📁 モデルグループ B
    - └ 📁 モデルグループ C
    - └ 📁 モデルグループ D
- 解決後（整理された状態）
  - 📂 コンピュータビジョン（モデルレジストリコレクション）
    - └ 📁 モデルグループ A
    - └ 📁 モデルグループ B
  - 📂 自然言語処理（モデルレジストリコレクション）
    - └ 📁 モデルグループ C
  - 📂 音声認識（モデルレジストリコレクション）
    - └ 📁 モデルグループ D
- モデルアーティファクト
  - 機械学習モデルをトレーニングした結果として生成されるファイルのことです。
  - 具体的には、以下のようなものが含まれます：
  - トレーニング済みモデルの重み（パラメータ） → .tar.gz などの圧縮ファイル
  - モデルのメタデータ → モデルの構成情報（例：model.pkl, model.json）
  - 前処理・後処理のスクリプト → inference.py など

## 22.

- Amazon SageMaker Studio Classic は、ML モデルの構築、トレーニング、デプロイメントを一元化して管理できるツールであり、モデル展開パイプラインを簡単に作成することが可能です。

## 23.

- 予測に使用された基礎となる顧客データで概念ドリフトが発生した
  - 概念ドリフトとは、トレーニングデータと本番環境でのデータの分布やパターンが異なることで発生する現象です。
  - たとえば、顧客の行動や市場のトレンドが変化することで、モデルの予測精度が低下する場合があります。
  - Amazon Sage Maker Model Monitor は F1 スコアの逸脱を検出するための強力なツールですが、基礎となるデータに変化が生じると、モデルが適切に機能しなくなる可能性があります。
  - このシナリオでは、概念ドリフトが F1 スコアの低下の主な原因として最も妥当です。
- モデルは、元のベースライン・データのすべてのパターンを捕捉するには十分に複雑ではなかった。
  - モデルの複雑さが十分でない場合、トレーニング時点で精度が低下します。
- 「元のベースライン・データには、欠損値というデータ品質の問題があった。」は不正解です。
  - データ品質の問題（例：欠損値）は、モデルのトレーニング時点で影響を及ぼすため、数か月間スコアが安定していたこととは矛盾します。
- 「ベースラインの計算中に、誤った正解ラベルが Model Monitor に提供された。」は不正解です。
  - ベースラインが正しく計算されていない場合、初期の検出結果が不正確になります。
    = しかし、本問では「数か月間は F1 スコアが安定していた」とあるため、この理由は矛盾しています。

XGBoost 予測モデルと F1 スコア

1. XGBoost 予測モデルとは？  
   XGBoost（eXtreme Gradient Boosting）は、機械学習の勾配ブースティングを効率的に実装したライブラリです。

- 決定木（Decision Tree）を複数組み合わせて高精度な予測を行う
- 分類（Yes/No）・回帰（数値予測）どちらにも対応可能
- 高速・高精度で、Kaggle などのデータ分析コンペでも人気

XGBoost 予測モデルとは、XGBoost を使ってデータを学習し、新しいデータに対して予測を行う機械学習モデルのことです。

例：「この顧客が商品を購入するか？」

- 入力データ：年齢、性別、過去の購入履歴
- モデルの出力：購入する確率（例：85%）

2. F1 スコアとは？（分類モデルの評価指標）  
   XGBoost などの分類モデル（Yes/No を判定するモデル）を評価する指標の一つが F1 スコアです。

正解率（Accuracy）だけでは不十分な場合があります。  
例えば、詐欺検出モデルで 99%のデータが「正常」だった場合、すべてを「正常」と予測すれば Accuracy は 99%になりますが、これは実用的ではありません。  
そこで、Precision（適合率）と Recall（再現率）のバランスを考慮する F1 スコアが有用です。

F1 スコアの計算方法  
F1 = (2 × Precision × Recall) / (Precision + Recall)

Precision（適合率）：予測が「正」とされたもののうち、本当に正解だった割合

- 例：詐欺検出モデルが「詐欺」と予測した 10 件のうち、8 件が本当に詐欺 → Precision = 8/10 = 0.8

Recall（再現率）：実際に「正」だったもののうち、正しく予測できた割合

- 例：本当は詐欺だった 15 件のうち、8 件を正しく詐欺と予測 → Recall = 8/15 = 0.533

F1 スコアは Precision と Recall のバランスを考えながら、どのくらい正確に予測できるかを測る指標です。  
値が 1 に近いほどモデルの性能が良く、0 に近いと精度が低いことを意味します。

F1 スコアの例

- F1 スコア = 0.9 → 高精度なモデル
- F1 スコア = 0.5 → 精度が低め

3. まとめ  
   | 用語 | 説明 |  
   |------|------|  
   | XGBoost 予測モデル | 勾配ブースティングを活用した高精度な機械学習モデル |  
   | F1 スコア | Precision と Recall のバランスを考慮した分類モデルの評価指標 |

分類モデルを作ったら、F1 スコアをチェックするとモデルの精度が分かります。

- 勾配 → 誤差を減らす方向を見つける。坂の傾き。
- ブースティング → 弱いモデルを組み合わせて強化する

## 25.

- プロビジョンド並列処理を使用して Amazon SageMaker Serverless Inference を使用する
  - Amazon SageMaker Serverless Inference は、AWS が基盤となるインフラストラクチャの管理を自動化し、リクエストに基づいてスケーリングを行います。
  - Sagemaker を使ったリアルタイム推論をサーバレスで行えるサービスです。リクエストが来たときだけ AWS 側でサーバを立ち上げて推論処理をしてくれるので余計なコストの削減につながります。
  - 特に、プロビジョンド並列処理を使用することで、分析期間中に複数のリクエストが発生しても迅速に対応できます。
  - このソリューションは、設定の手間を減らし、パフォーマンス要件を満たすのに最適です

## 26.

- ある企業は、Amazon SageMaker エンドポイントで運用中の不正検出モデルを利用しています。この企業では、新しいモデルバージョンを開発中であり、運用中のエンドユーザーへの影響を避けながらライブデータを使用して新しいモデルのパフォーマンスを評価する必要があります。これらの要件を満たすソリューションはどれでしょうか？

- Amazon SageMaker エンドポイントとは？
  - 機械学習モデルをデプロイ（本番環境で運用）するためのサービスです。
  - SageMaker でトレーニングしたモデルをエンドポイントにデプロイすると、API のようにリクエストを受け付け、リアルタイムで予測を行うことができます。
- エンドポイントの主な特徴
  - リアルタイム推論
    - クライアント（アプリや Web サービスなど）がエンドポイントにリクエストを送ると、機械学習モデルが即座に予測を返します。
    - 例：「この取引は不正か？」というリクエストに対して、「不正の確率 92%」といったレスポンスを返す。
- スケーラビリティ
  - トラフィックに応じてインスタンスを自動で増減できるため、大量のリクエストにも対応可能。
- A/B テストやシャドーデプロイ
  - 本番環境のモデルを入れ替える際に、新しいモデルの精度を安全にテストできる（今回のシナリオに関係）。
- この企業は、現在運用中の不正検出モデルに影響を与えず、新しいモデルをライブデータで評価したい。
  - 適した方法の一つが、シャドーデプロイメント（Shadow Deployment）を利用すること。
- シャドーデプロイメントの流れ

  1. 現在のエンドポイントはそのまま運用し、エンドユーザーには影響を与えない。
  2. ライブデータ（実際のリクエスト）を新しいモデルにも送るが、結果はエンドユーザーに表示しない。
  3. 新旧モデルの出力を比較し、新しいモデルの精度を評価する。
  4. 新モデルの精度が十分なら、本番環境のモデルを切り替える。

- 不正解の理由「SageMaker デバッガーを設定し、カスタムルールを作成する。」
  - SageMaker デバッガーは、トレーニングジョブの問題を検出するためのツールであり、運用中のモデルのパフォーマンス評価には適していません。
  - この要件では、新しいモデルをライブデータで評価する必要があるため、不適切です。
- 「一括トラフィックシフトを使用してブルー/グリーンデプロイメントを設定する。」
  - 一括トラフィックシフトは、新しいモデルにトラフィックを一度に切り替えるデプロイメント方法です。
  - この方法では、モデルのテスト中に運用トラフィックが直接影響を受ける可能性があり、要件を満たしません。
- 「カナリートラフィックシフトを使用してブルー/グリーンデプロイメントを設定する。」
  - カナリートラフィックシフトは、トラフィックを徐々に新しいモデルに切り替えるデプロイメント方法です。
  - この方法も本番トラフィックを使用して新しいモデルを評価するため、エンドユーザーに影響を与える可能性があります。
  - そのため、ライブデータを使用しつつもエンドユーザーに影響を与えないという要件を満たしません。

## 27.

- 正解は「平均絶対誤差（MAE）」です。
  - MAE は、予測値と実際の値の絶対的な差を平均したものであり、回帰問題におけるモデルの性能を評価するのに適したメトリックです。
  - このシナリオでは、アパートの価格という数値を予測するため、分類問題のメトリック（正確度、AUC、F1 スコア）は適用できません。
  - MAE は、誤差が小さいほど優れたモデルであることを示し、解釈が直感的であるため、多くの回帰タスクで使用されます。
- 「正確度」は不正解です。
  - 正確度は分類問題の評価指標であり、正しく予測されたデータポイントの割合を示します。
  - しかし、価格予測のような回帰タスクでは適用できません。
  - 正確度では、予測値が正解にどれだけ近いかを評価できないため、不適切です。
- 「ROC 曲線下面積（AUC）」は不正解です。
  - AUC は、分類モデルの性能を評価する指標であり、特にクラス間の分離能力を測定します。
  - この問題は回帰タスクであり、AUC を使用する意味がありません。
  - 価格予測の精度を測るためには適していません。
- 「F1 スコア」は不正解です。
  - F1 スコアは、分類問題で正確さと完全性のバランスを評価する指標です。
  - この指標は二値または多クラス分類問題に使用され、回帰タスクである価格予測には適していません。

回帰（Regression）と分類（Classification）の違い

機械学習の問題は、大きく回帰（Regression）と分類（Classification）の 2 種類に分けられます。

### 1. 回帰（Regression）とは？

👉 数値を予測するタスク

- 例えば、アパートの価格を予測する場合、「200 万円」「300 万円」「450 万円」など連続した数値を予測する必要があります。
- 回帰問題では、「正解にどれくらい近いか？」が重要になります。
- そのため、評価指標には平均絶対誤差（MAE）や平均二乗誤差（MSE）などが使われます。

✅ 回帰の例

- アパートの価格予測：「この物件の価格は ○○ 万円くらい？」
- 気温予測：「明日の最高気温は何度？」
- 売上予測：「来月の売上はいくらになる？」

### 2. 分類（Classification）とは？

👉 カテゴリ（Yes/No・クラス）を予測するタスク

- 例えば、「このメールはスパムか？」といった判定は「スパム or スパムでない」の 2 つのカテゴリに分類されます。
- また、「A、B、C の 3 つのクラスに分類する」といった多クラス分類もあります。
- 正解・不正解が明確なので、評価指標には正確度（Accuracy）、AUC、F1 スコアなどが使われます。

✅ 分類の例

- スパムメール判定：「このメールはスパムか？（Yes / No）」
- 病気診断：「この患者は病気か？（Yes / No）」
- 手書き数字認識：「この画像の数字は 0〜9 のどれか？」

### 3. まとめ（回帰 vs 分類）

| 項目         | 回帰（Regression） | 分類（Classification）   |
| ------------ | ------------------ | ------------------------ |
| 予測するもの | 数値（連続値）     | カテゴリ（離散値）       |
| 例           | 価格予測、売上予測 | スパム判定、病気診断     |
| 評価指標     | MAE、MSE           | Accuracy、AUC、F1 スコア |

アパートの価格予測は数値を予測する回帰問題なので、「MAE」が正しい評価指標になります。  
「正確度」「AUC」「F1 スコア」は分類問題で使われる指標なので、価格予測には適しません。

Accuracy（正確度）、AUC（ROC 曲線下面積）、F1 スコアの違い

これらはすべて分類問題（Classification）の評価指標ですが、それぞれの特徴や適用シーンが異なります。

---

### 1. Accuracy（正確度）

👉 全体のうち、正しく分類できた割合

📌 計算式  
\[
Accuracy = \frac{\text{正しく分類された数}}{\text{全データ数}}
\]

✅ 適している場面

- クラスのバランスが取れているとき（例：陽性 50%、陰性 50%）
- 全体の正解率を単純に知りたいとき

🚨 注意点

- クラスの比率が偏っていると意味がなくなる（例：99%が陰性なら、全部「陰性」と予測しても Accuracy 99%）。

✅ 例（ガン検査モデル）

- データ 100 件（ガンあり 10 件、ガンなし 90 件）
- モデルの予測結果
  - 90 件を「ガンなし」と予測（正解）
  - 10 件のうち 2 件だけ「ガンあり」と予測（正解）、残り 8 件は「ガンなし」と誤判定
- Accuracy = (90+2)/100 = 92%（でもガン患者 8 人を見逃してる…！）

---

### 2. AUC（ROC 曲線下面積）

👉 モデルが「陽性」と「陰性」をどれだけうまく区別できるかを測る指標

📌 ポイント

- しきい値を変えながら、全体の分類性能を評価
- AUC = 1 に近いほど、完璧に分類できるモデル
- 0.5 に近いと、ランダム予測と同じ（意味がない）

✅ 適している場面

- クラスのバランスが偏っているとき
- モデルの全体的な識別能力を見たいとき

🚨 注意点

- AUC が高くても、適切なしきい値を設定しないと実用的でない。

✅ 例（ガン検査モデル）

- AUC = 0.95 なら、高い精度でガンと非ガンを区別できる
- AUC = 0.5 なら、ランダムに予測してるのと同じ（ダメなモデル）

---

### 3. F1 スコア

👉 Precision（適合率）と Recall（再現率）のバランスを考慮した指標

📌 計算式  
\[
F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}
\]

Precision（適合率） = 陽性と予測した中で、本当に陽性だった割合  
\[
Precision = \frac{\text{真陽性}}{\text{真陽性} + \text{偽陽性}}
\]  
Recall（再現率） = 実際に陽性だったものの中で、どれだけ正しく検出できたか  
\[
Recall = \frac{\text{真陽性}}{\text{真陽性} + \text{偽陰性}}
\]

✅ 適している場面

- Precision と Recall のバランスをとりたいとき
- 不正検知、ガン検査など、偽陰性を減らしたいタスク

🚨 注意点

- Precision と Recall のどちらを重視するかはタスクによる。

✅ 例（ガン検査モデル）

- Precision = 0.8、Recall = 0.5 の場合
  - 「ガン」と予測したものの 80%は本当にガンだった（Precision = 0.8）
  - でも、実際のガン患者のうち 50%しか検出できていない（Recall = 0.5）
- F1 スコア = 0.62（バランスを考慮）

---

### 4. Accuracy / AUC / F1 の使い分け

| 指標                  | 使いやすさ                                 | いつ使うべきか                                          | 注意点                           |
| --------------------- | ------------------------------------------ | ------------------------------------------------------- | -------------------------------- |
| Accuracy（正確度）    | 簡単                                       | クラスのバランスが良いとき                              | クラスの偏りがあると意味がない   |
| AUC（ROC 曲線下面積） | モデルの全体的な性能を測るのに便利         | クラスのバランスが悪いとき、全体の区別能力を見たいとき  | しきい値の影響を考慮する必要あり |
| F1 スコア             | Precision と Recall のバランスを考慮できる | クラスの偏りがある & 偽陽性・偽陰性どちらも気になるとき | 計算が少し複雑                   |

---

### 5. まとめ

- Accuracy → クラスバランスが取れているなら OK
- AUC → クラスバランスが偏っていても使える
- F1 スコア → 偽陽性・偽陰性のバランスを重視するなら

例えば…

- ガン検査 → F1 スコア or Recall（見逃しを減らしたい）
- スパムフィルター → Precision（誤ってスパム扱いするのを減らしたい）
- クラスバランスが偏っている不正検知 → AUC

状況に応じて、適切な指標を選ぶのが重要！

## 29.

### バリアント（プロダクションバリアント）とは？

Amazon SageMaker では、1 つのエンドポイントに複数のモデルをデプロイできます。そのとき、各モデルを「バリアント（Production Variant）」と呼びます。

プロダクションバリアントの特徴

- 1 つのエンドポイントで複数のモデルを提供できる
- トラフィックの割合（ウェイト）を設定できる（例：90%を既存モデル、10%を新モデルに送る）
- トラフィックの分割を簡単に管理できる（新モデルの性能を試しながら徐々に移行可能）

### オンライン検証とは？

オンライン検証（Online Validation）は、本番環境でモデルの性能をテストする方法の 1 つです。

❌ オフライン検証（Offline Validation）

- 事前に用意したデータでモデルを評価（学習時のデータを使用）
- 過去のデータしか使えないため、実際の動作を完全には再現できない

✅ オンライン検証（Online Validation）

- リアルな本番データを使ってモデルを評価
- エンドユーザーが使用するデータで検証できる
- ただし、本番環境への影響を最小限に抑えながら実施する必要がある

### 今回のケース

企業が新しい ML モデルを試したいが、本番のすべてのトラフィックを新モデルに送るのはリスクがある。  
そこで SageMaker のバリアントを使い、新モデルにトラフィックの 10%だけを送ることでオンライン検証を行う。  
→ 問題なく動作すれば、新モデルの割合を増やして完全移行することが可能！

---

### まとめ

| 用語                     | 説明                                                                                        |
| ------------------------ | ------------------------------------------------------------------------------------------- |
| プロダクションバリアント | SageMaker で 1 つのエンドポイントに複数のモデルをホストし、トラフィックの割合を調整する機能 |
| オンライン検証           | 本番データを使って新しいモデルの性能をテストする方法                                        |

新モデルのバリアントの重みを 0.1（=10%のトラフィック）に設定することで、安全にオンライン検証ができる！
