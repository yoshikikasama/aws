# CloudTech Study

## 41.

### 「少数派クラスを簡単にオーバーサンプリング」ってどういうこと？

これは、「データの中で少ないクラス（例：詐欺取引、がん患者など）を増やすことで、クラスのバランスを取る」 という意味です。

---

### 1. なぜオーバーサンプリングが必要？

機械学習のモデルは、データの分布が偏っている（不均衡データ）と、少数派クラスをうまく学習できないことがある からです。

例えば、クレジットカードの不正取引データを考えてみましょう。

| クラス   | データ数  | 割合 |
| -------- | --------- | ---- |
| 正常取引 | 99,000 件 | 99%  |
| 不正取引 | 1,000 件  | 1%   |

👉 このままだと、モデルは「ほぼ全部正常」と予測するだけで 99%の正解率 を出せてしまう。  
📌 でも、それでは不正取引（本当に見つけるべきデータ）をほとんど見逃してしまう！

そこで、「不正取引のデータを増やす（オーバーサンプリング）」ことで、バランスを取る必要があります。

---

### 2. オーバーサンプリングって何？

📌 少数派クラス（この例では「不正取引」）のデータを人工的に増やす方法のこと。

主な方法：

- 単純な複製 → 既存のデータをそのままコピーして増やす。
- SMOTE（Synthetic Minority Over-sampling Technique） → 既存のデータに基づいて新しいデータを合成する。
- Data Augmentation（データ拡張） → 少数派クラスのデータを加工して新しいデータを作る。

例えば：

- 1,000 件しかない不正取引データを 10,000 件に増やす ことで、モデルが「不正取引」のパターンをちゃんと学習できるようにする。

---

### 3. SageMaker Data Wrangler でオーバーサンプリングするメリット

通常、オーバーサンプリングをするには Python コードを書いて `imbalanced-learn` ライブラリを使ったりする必要があるけど、Amazon SageMaker Data Wrangler なら、コードを書かずに（ノーコード）簡単にできる！

SageMaker Data Wrangler の「バランスデータ操作（Balance Data Transformations）」を使うと：
✅ GUI でクリックするだけでオーバーサンプリングができる！  
✅ データの可視化や前処理も一緒にできる！  
✅ 運用コストを減らせる！（手作業を最小限にできる）

---

### 4. まとめ

- 不均衡データでは、少数派クラス（例：詐欺、不良品、病気）を増やす（オーバーサンプリング）ことで、モデルがちゃんと学習できるようにする！
- SageMaker Data Wrangler を使えば、オーバーサンプリングをノーコードで簡単に実行できる！
- 「バランスデータ操作」を使うと、クラス間のバランスを取るのが楽になる！

だから、「少数派クラスを簡単にオーバーサンプリングする」という表現になっている！

## 42.

- Redshift の動的データマスキングポリシーを設定して、クエリ実行時に機密データをデータサイエンティストと共有する方法を制御する
  - 正解の理由動的データマスキング（DDM）は、クエリの実行時に特定の条件に基づいてデータの表示方法を制する機能を提供します。
  - このソリューションは、データを変換したり新しいデータセットを保存することなく、既存のデータソースに基づいて機密データを保護できます。
  - これにより、データサイエンティストは必要な情報にアクセスでき、機密データが直接公開されるリスクを最小限に抑えることができます。
  - また、Redshift の機能を活用するため、追加のセットアップやコストが不要で、運用効率も高いです。
- 不正解の理由「データベースの上位にマスキングロジックを持つマテリアライズドビューを作成する。データサイエンティストに必要な読み取り権限を付与する。」
  - マスキングロジックを持つマテリアライズドビューを作成する場合、ビューの更新や管理が必要になります。
  - また、マスキングロジックをデータに適用する過程で追加の複雑さが生じるため、運用効率が低下します

## 43.

- Amazon SageMaker 非同期推論エンドポイントとスケーリングポリシーを作成する。各画像に対して推論リクエストを行うスクリプトを実行する
  - 非同期推論エンドポイントは、バッチデータ処理に最適化されており、リアルタイムでの応答を必要としないユースケースに適しています。
  - また、スケーリングポリシーを設定することで需要の変化に柔軟に対応できます。
  - 運用のオーバーヘッドが少なく、処理リクエストが高負荷になった場合にも自動的にリソースを拡張して対応できる点が大きな利点です。

## 44.

### 1. まず、「確率的勾配降下法（SGD）」って何？

簡単に言うと、モデルが「より良い予測」をするために、少しずつ調整する方法 のこと。

モデルは最初、適当な予測をするけど、学習データを見ながら少しずつ改善する。  
この「改善のステップ」の大きさを決めるのが、「学習率（Learning Rate）」 というパラメータ。

- 学習率が大きい ＝ 一気に改善しようとする（でも行き過ぎる可能性あり）
- 学習率が小さい ＝ ゆっくり少しずつ改善（時間はかかるけど安定）

イメージするとこんな感じ 👇

- 大きすぎる学習率 ＝ ジャンプしながら階段を降りる → 行き過ぎたり、戻ったりしてバランスを崩す
- 小さすぎる学習率 ＝ 1 ミリずつゆっくり歩く → 確実だけど時間がかかる

---

### 2. 「損失（Loss）」って何？

機械学習モデルは、何かを予測するけど、最初はだいたい 間違った予測 をする。  
この「間違いの大きさ」を 「損失（Loss）」 という。

モデルの目標は：
✔ 損失（Loss）をできるだけ小さくすること！

---

### 3. 今回の問題は何が起きてるの？

- 「損失が振動している」  
  → 学習してるはずなのに、損失が減ったり増えたりを繰り返してる。
- 「トレーニング損失も検証損失も高い」  
  → そもそも、モデルの精度が上がっていない。

📌 このパターンは、「学習率が大きすぎる」典型的なケース！

---

### 4. じゃあ、解決策は？

✅ 「学習率を減らす」

- 大きすぎる学習率を小さくすれば、モデルが少しずつ安定して学習できる。
- 「行き過ぎたり戻ったり」が減るので、損失がスムーズに減るようになる。

📌 これが正解！

---

### 5. 他の選択肢がダメな理由

- 「アーリーストップを導入する」 ❌

  - アーリーストップは、「学習しすぎて逆に悪くなる（過学習）」のを防ぐもの。
  - 今回はそもそも学習が安定していないので、意味がない！

- 「テストセットのサイズを増やす」 ❌

  - テストデータを増やしても、そもそもの学習がうまくいってないので効果なし！

- 「学習率を増やす」 ❌
  - すでに大きすぎて振動してるのに、さらに大きくしたら、もっと悪化する！

---

### 6. 超シンプルまとめ

1. SGD（確率的勾配降下法） = モデルの予測を少しずつ改善する方法。
2. 学習率（Learning Rate） = 改善のステップの大きさ。
   - 大きすぎると、行き過ぎたり戻ったりして安定しない！
3. 今回の問題は、損失（Loss）が減ったり増えたりしてる（振動してる）
   - これは、学習率が大きすぎる典型的なケース！
4. 解決策は「学習率を減らす」こと！ ✅

## 45.

- SageMaker の推論タイプ

  - リアルタイム推論
    - 60 秒以内に終わる処理、データサイズも 6MB 以下
    - 裏でサーバーが常時起動するため、デプロイ中ずっと料金が発生（高価）
  - サーバーレス推論
    - 60 秒以内に終わる処理、データサイズも 6MB 以下
    - たまにしか推論しない場合、リクエスト数が予測できない場合に当てはまる
    - レイテンシ要件が厳しい場合、コールドスタートがあるため向かないケースも
  - 非同期推論
    - 1 時間以内に終わる処理、データサイズは 1GB まで
  - バッチ推論
    - 数日の処理時間をサポート、データサイズも 1GB 以上に対応（最小でも 100MB 必要

## 47.

- デプロイされたモデルで SageMaker Clarify を使用する。
  - SageMaker Clarify は、ML モデルの予測の公平性や説明可能性を評価するために設計されたツールです。
  - このソリューションを使用することで、感情分析モデルの予測がどのように行われるか、どの特徴が結果に影響を与えているかを視覚的かつ詳細に説明できます。
  - これにより、利害関係者にモデルの動作をわかりやすく伝えられます。
- デプロイされたモデルで SageMaker Model Monitor を使用する。は不正解です。
  - SageMaker Model Monitor は、モデルのデータドリフトや品質の監視に適していますが、モデルの予測の詳細な説明には適していません
  - データドリフトとは、モデルが学習したデータと、実際の運用環境で使われるデータが変化してしまうこと を指します。
  - これが起こると、モデルの予測精度が低下してしまう 可能性があります。

## 48.

OK！シンプルにわかりやすく説明するね！

---

### 1. 何が起きているのか？

企業は 「不正検出モデル」 を作りたい。  
👉 クレジットカードの取引を「不正 or 正常」に分類するモデルを作っている。

でも、今のモデルには 問題がある！
✅ トレーニングデータ ではうまく不正を見つけられる。  
❌ 新しいデータ（これまで見たことのない取引）ではうまく検出できない。

📌 つまり、「過学習（Overfitting）」している！  
📌 「学習したデータにピッタリすぎて、新しいデータに対応できない」状態。

---

### 2. 「過学習（Overfitting）」とは？

機械学習モデルは、学習データを使ってルールを学ぶ。  
でも、学習しすぎると「クセ」まで覚えてしまい、新しいデータに弱くなる！

例えば：

- テストの丸暗記をした学生 は、同じ問題なら解けるけど、新しい問題に対応できない！
- 柔軟に考えられる学生 は、新しい問題も解ける！

👉 過学習したモデルは、丸暗記した学生みたいなもの！  
👉 新しいデータに対応できるようにするには、「学習のしすぎ」を防ぐ必要がある！

---

### 3. 「max_depth」って何？

このモデルは XGBoost（エックスジーブースト） という手法を使っている。  
XGBoost は 決定木（Decision Tree） をたくさん組み合わせたモデル。

📌 決定木とは？  
決定木は、「はい or いいえ」でデータを分類する仕組み。

例：  
💳 クレジットカードの取引データ

1. 取引金額が $10,000 以上？ → はい → 不正かも！
2. 国が違う？（日本のカードなのにアメリカで使用） → はい → 不正かも！
3. 時間帯が夜中？ → はい → 不正かも！

こんな感じでルールを作る。

📌 「max_depth」って何？  
👉 決定木の「深さ（Depth）」を決めるパラメータ！  
👉 深くなればなるほど、細かいルールまで覚えられる！

---

### 4. 「max_depth を減らす」とは？

max_depth を大きくすると…
✔ 学習データにピッタリなルールを作れる（でも新しいデータに弱い）  
✔ 過学習しやすくなる！

max_depth を小さくすると…
✔ モデルが「ほどよく」学習する（新しいデータにも対応しやすい）  
✔ 過学習を防げる！

📌 だから、「max_depth を減らす」が正解！

---

### 5. 他の選択肢が間違いな理由

- 「学習率を増やす。」 ❌

  - 学習率を増やすと、トレーニングの進みが速くなるけど、ちゃんと学習できないことがある。
  - 新しいデータに対して改善されるとは限らない！

- 「トレーニングデータセットからいくつかの無関係な特徴を削除する。」 ❌

  - 無関係なデータを減らすのは有効なこともあるけど、
  - この問題の原因は「過学習」だから、max_depth の調整が優先！

- 「max_depth の値を増やす。」 ❌
  - max_depth を増やすと、もっと過学習が進んでしまう！
  - トレーニングデータには強くなるけど、新しいデータにはさらに弱くなる！
